# System Patterns: TinyAgent
*Version: 1.2*
*Created: 2025-05-31*
*Last Updated: 2025-06-01*

## Architecture Overview
TinyAgent employs a modular architecture centered around a core agent engine built with Python and the `openai-agents-python` SDK. The agent operates on a ReAct (Reasoning and Acting) loop, with future provisions for a Reflect mechanism. A key architectural innovation is the **multi-model LLM support layer** using LiteLLM, which has been successfully implemented and enables seamless integration with 100+ LLM providers while maintaining a unified interface. Extensibility is achieved through configurable LLM providers, externalized prompts, and robust Model Context Protocol (MCP) integration for tool use. The initial interaction is via a Command-Line Interface (CLI).

## Key Components
- **Core Agent Engine:**
    - *Purpose:* Manages the main operational loop (ReAct), orchestrates calls to LLMs and MCP tools, and maintains conversational context.
    - *Details:* Built on `openai-agents-python`. Includes the `Runner.run()` for the basic loop.
    - *Status:* âœ… **Implemented and Working**
- **Multi-Model LLM Provider Module:**
    - *Purpose:* Abstracts interactions with Large Language Models through a dual-layer approach: OpenAI native models via standard client, and third-party models (Google, Anthropic, DeepSeek, etc.) via LiteLLM integration.
    - *Details:* Automatically detects model type and routes through appropriate client. Supports OpenRouter, direct provider APIs, and local models.
    - *Technical Stack:* 
      - OpenAI Agents SDK native support for OpenAI models
      - LiteLLM for 100+ third-party model providers  
      - Automatic model prefix detection and routing
    - *Status:* âœ… **Implemented and Tested** - Successfully routes Google Gemini, Anthropic, DeepSeek models
- **Prompt Management Module:**
    - *Purpose:* Loads agent instructions and task-specific prompts from external files (e.g., `prompts/` directory).
    - *Details:* Allows behavior customization without code changes.
    - *Status:* âœ… **Implemented**
- **MCP Configuration & Integration Module:**
    - *Purpose:* Manages connections to MCP servers (defined in `mcp_servers.yaml`) and facilitates tool discovery and invocation by the agent.
    - *Details:* Leverages `openai-agents-mcp` extension or a custom solution. Supports stdio-based tools initially, with design for future HTTP/SSE tools.
- **MCP Tool Ecosystem:**
    - *Purpose:* Provides the set of external tools the agent can use to perform actions (e.g., file access, data processing, code analysis).
    - *Details:* Tools are exposed by MCP servers. Configuration allows for metadata and categorization.
- **Hierarchical Configuration System:**
    - *Purpose:* Manages complex configuration through a 4-tier hierarchy: Environment Variables (.env) > User Configuration (config/) > Profile Configurations (profiles/) > Default Configurations (defaults/)
    - *Details:* Supports environment variable substitution, profile-based deployment, and resource definition separation.
- **Command-Line Interface (CLI):**
    - *Purpose:* Provides the initial user interaction point for initiating tasks and receiving outputs.
- **Workflow Management Module (Future):**
    - *Purpose:* Placeholder for future advanced workflow orchestration logic.

## Design Patterns in Use
- **Modular Design:** The system is broken down into distinct modules (LLM, Prompts, MCP, Core Engine) with clear responsibilities to enhance maintainability and extensibility.
- **Configuration-Driven Design:** Key aspects like tool integration, LLM selection, and prompts are managed through external configuration files, promoting flexibility.
- **Abstraction Layer:** Used for LLM providers to decouple the core agent from specific LLM implementations.
- **ReAct (Reasoning and Acting):** The core operational paradigm for the agent, enabling it to reason about tasks and take actions.
- **Strategy Pattern (Implicit):** Different LLMs or prompt sets could be seen as different strategies for task execution, selectable via configuration.

## Data Flow (Example: Multi-Model LLM Request Processing)
1. **Initiation:** User initiates request via CLI, providing input (e.g., "generate document using Google Gemini").
2. **Model Detection & Routing:** TinyAgent analyzes the configured model name:
   - **Model Prefix Analysis:** Checks for third-party prefixes (`google/`, `anthropic/`, `deepseek/`, etc.)
   - **Routing Decision:** 
     - *OpenAI Models:* `gpt-4`, `gpt-3.5-turbo` â†’ Route to OpenAI Client
     - *Third-Party Models:* `google/gemini-2.0-flash-001` â†’ Route to LiteLLM
     - *Unknown Models:* Default to OpenAI routing for backward compatibility
3. **LLM Client Initialization:**
   - **OpenAI Route:** Create standard OpenAI client with OpenAI Agents SDK
   - **LiteLLM Route:** Create `LitellmModel` wrapper with provider-specific configuration
4. **Analysis & Planning:** Agent (using the routed LLM) analyzes input and plans steps.
5. **Execution (Action & Tool Use):**
    - Agent may call MCP tools (e.g., `file_reader_tool`, `text_formatter_tool`) via MCP Integration Module
    - Agent sends requests to the configured LLM via the appropriate client
    - LiteLLM handles provider-specific API differences transparently
6. **Observation & Assembly:** Agent receives tool outputs and LLM responses.
7. **Content Assembly:** Agent assembles the final output.
8. **Error Handling & Fallback:** 
   - *Provider Errors:* LiteLLM handles retries and provider-specific error codes
   - *Model Errors:* Graceful degradation to alternative models if configured
9. **Output:** Agent outputs result to user via CLI.

## Key Technical Decisions
- **Use `openai-agents-python` SDK:** Provides a foundational framework for agent development.
- **Adopt Multi-Model LLM Strategy via LiteLLM:** 
  - *Rationale:* Enables support for 100+ LLM providers while maintaining OpenAI Agents SDK compatibility
  - *Implementation:* Dual-layer approach - native OpenAI client for OpenAI models, LiteLLM for third-party models
  - *Benefits:* Provider flexibility, cost optimization, model experimentation without architectural changes
- **Implement Automatic Model Routing:**
  - *Strategy:* Detect model prefix (e.g., `google/`, `anthropic/`, `deepseek/`) to determine routing strategy
  - *OpenAI Models:* Use standard OpenAI client with OpenAI Agents SDK
  - *Third-Party Models:* Route through LiteLLM with `LitellmModel` wrapper
  - *Fallback:* Standard model names default to OpenAI routing
- **Adopt MCP for Tooling:** Enables a standardized and extensible way to integrate tools.
- **Implement Hierarchical Configuration Architecture:**
  - *Design Principle:* DRY (Don't Repeat Yourself) with resource definition separation
  - *Priority System:* Environment Variables > User Config > Profiles > Defaults
  - *Benefits:* Environment-agnostic deployment, secure credential management, easy customization
- **Externalize Configuration:** LLMs, Prompts, and MCP tools are configured externally for flexibility.
- **Initial CLI Focus:** Prioritizes core functionality over a GUI for the first version.
- **Stdio for Initial MCP Tools:** Simplifies local tool integration.
- **ReAct Loop as Core:** Provides a robust pattern for agent operation.
- **Environment Variable Integration:** Secure credential management and environment-specific configuration via .env files.

## Component Relationships
```mermaid
graph TD
    User --> CLI;
    CLI --> CoreAgentEngine;
    CoreAgentEngine --> LLMRouter[LLM Router/Detector];
    LLMRouter --> |OpenAI Models| OpenAIClient[OpenAI Client];
    LLMRouter --> |Third-Party Models| LiteLLM[LiteLLM Client];
    OpenAIClient --> OpenAIService[OpenAI API];
    LiteLLM --> ThirdPartyServices[Google/Anthropic/DeepSeek APIs];
    CoreAgentEngine --> PromptManagementModule;
    CoreAgentEngine --> MCPIntegrationModule;
    
    %% Configuration Layer
    ConfigManager[Configuration Manager] --> EnvironmentVars[Environment Variables (.env)];
    ConfigManager --> UserConfig[User Config (config/)];
    ConfigManager --> Profiles[Profiles (profiles/)];
    ConfigManager --> Defaults[Defaults (defaults/)];
    
    %% LLM Configuration
    LLMRouter --> ConfigManager;
    ConfigManager --> LLMProviders[llm_providers.yaml];
    
    %% Other Configuration
    PromptManagementModule --> PromptsDir[prompts/ directory];
    MCPIntegrationModule --> MCPConfig[mcp_servers.yaml];
    MCPIntegrationModule --> MCPTools[MCP Tool Servers (Stdio/HTTP)];
    
    %% Output
    CoreAgentEngine --> User;
    
    %% Styling
    classDef newComponent fill:#e1f5fe;
    classDef configComponent fill:#f3e5f5;
    class LLMRouter,LiteLLM newComponent;
    class ConfigManager,EnvironmentVars,UserConfig,Profiles,Defaults configComponent;
```

## Configuration Architecture Design

### Current Issues Analysis

**Problems with Current Configuration:**
1. **Duplication**: LLM settings exist in both `agent_config.yaml` and `llm_config.yaml`
2. **No Clear Hierarchy**: Multiple config files with unclear precedence
3. **Mixed Concerns**: Main config and examples mixed together
4. **Inconsistent Naming**: `mcp_agent.config.yaml` vs `agent_config.yaml`
5. **No Environment Support**: Limited .env file integration
6. **Complex Structure**: Difficult to understand relationships

### Proposed Configuration Architecture

#### 1. Configuration Hierarchy (æŒ‰ä¼˜å…ˆçº§æ’åº)

```
1. Environment Variables (.env file)     [HIGHEST PRIORITY]
2. User Configuration (config/)          [HIGH PRIORITY]
3. Profile Configurations (profiles/)    [MEDIUM PRIORITY]  
4. Default Configurations (defaults/)    [LOWEST PRIORITY]
```

#### 2. File Structure

```
tinyagent/configs/
â”œâ”€â”€ defaults/                    # é»˜è®¤é…ç½® (åªè¯»ï¼Œéšä»£ç åˆ†å‘)
â”‚   â”œâ”€â”€ agent.yaml              # ä»£ç†é»˜è®¤è®¾ç½®
â”‚   â”œâ”€â”€ llm_providers.yaml      # LLMæä¾›å•†é»˜è®¤é…ç½®
â”‚   â””â”€â”€ mcp_servers.yaml        # MCPæœåŠ¡å™¨é»˜è®¤é…ç½®
â”œâ”€â”€ profiles/                    # é¢„è®¾é…ç½®æ–‡ä»¶ (ç¤ºä¾‹å’Œæ¨¡æ¿)
â”‚   â”œâ”€â”€ development.yaml        # å¼€å‘ç¯å¢ƒé…ç½®
â”‚   â”œâ”€â”€ production.yaml         # ç”Ÿäº§ç¯å¢ƒé…ç½®
â”‚   â”œâ”€â”€ openrouter.yaml         # OpenRouterç¤ºä¾‹
â”‚   â””â”€â”€ local_llm.yaml          # æœ¬åœ°LLMç¤ºä¾‹
â”œâ”€â”€ config/                      # ç”¨æˆ·è‡ªå®šä¹‰é…ç½® (ä¼˜å…ˆçº§æœ€é«˜)
â”‚   â”œâ”€â”€ agent.yaml              # ç”¨æˆ·ä»£ç†é…ç½®
â”‚   â”œâ”€â”€ llm.yaml                # ç”¨æˆ·LLMé…ç½®
â”‚   â””â”€â”€ mcp.yaml                # ç”¨æˆ·MCPé…ç½®
â””â”€â”€ .env                         # ç¯å¢ƒå˜é‡ (æ•æ„Ÿä¿¡æ¯)
```

#### 3. Configuration Loading Logic

```python
# é…ç½®åŠ è½½ä¼˜å…ˆçº§é€»è¾‘
def load_configuration():
    config = {}
    
    # 1. åŠ è½½é»˜è®¤é…ç½®
    config.update(load_defaults())
    
    # 2. åŠ è½½é€‰å®šçš„é…ç½®æ–‡ä»¶ (å¦‚æœæŒ‡å®š)
    if profile:
        config.update(load_profile(profile))
    
    # 3. åŠ è½½ç”¨æˆ·é…ç½® (è¦†ç›–é»˜è®¤å’Œé…ç½®æ–‡ä»¶)
    config.update(load_user_config())
    
    # 4. åº”ç”¨ç¯å¢ƒå˜é‡ (æœ€é«˜ä¼˜å…ˆçº§)
    config.update(apply_env_vars(config))
    
    return config
```

### 4. Configuration Schema Design

#### 4.1 Master Configuration Structure

```yaml
# config/agent.yaml - ä¸»é…ç½®æ–‡ä»¶
agent:
  name: "TinyAgent"
  profile: "development"  # å¯é€‰: æŒ‡å®šè¦åŠ è½½çš„profile
  instructions_file: "prompts/default_instructions.txt"
  max_iterations: 10

llm:
  provider_config_file: "config/llm.yaml"  # å¼•ç”¨LLMé…ç½®æ–‡ä»¶
  active_provider: "openai"                # å½“å‰æ´»è·ƒçš„æä¾›å•†

mcp:
  server_config_file: "config/mcp.yaml"    # å¼•ç”¨MCPé…ç½®æ–‡ä»¶
  auto_discover: true                       # è‡ªåŠ¨å‘ç°MCPæœåŠ¡å™¨

logging:
  level: "${LOG_LEVEL:INFO}"
  format: "structured"
  file: "${LOG_FILE:}"

environment:
  env_file: ".env"                          # .envæ–‡ä»¶è·¯å¾„
  env_prefix: "TINYAGENT_"                 # ç¯å¢ƒå˜é‡å‰ç¼€
```

#### 4.2 LLM Provider Configuration

```yaml
# defaults/llm_providers.yaml - LLMæä¾›å•†é…ç½®
providers:
  # OpenAI Native Models (ä½¿ç”¨æ ‡å‡†OpenAI Client)
  openai:
    model: "${OPENAI_MODEL:gpt-4}"
    api_key_env: "OPENAI_API_KEY"
    base_url: "${OPENAI_BASE_URL:https://api.openai.com/v1}"
    max_tokens: 2000
    temperature: 0.7
    client_type: "openai"  # æ˜¾å¼æŒ‡å®šä½¿ç”¨OpenAI client

  # Third-Party Models via OpenRouter (ä½¿ç”¨LiteLLM)
  openrouter:
    model: "${OPENROUTER_MODEL:google/gemini-2.0-flash-001}"
    api_key_env: "OPENROUTER_API_KEY" 
    base_url: "https://openrouter.ai/api/v1"
    max_tokens: 2000
    temperature: 0.7
    client_type: "litellm"  # æ˜¾å¼æŒ‡å®šä½¿ç”¨LiteLLM
    extra_headers:
      HTTP-Referer: "${OPENROUTER_REFERER:https://github.com/your-org/tinyagent}"
      X-Title: "${OPENROUTER_TITLE:TinyAgent}"

  # Direct Third-Party Provider (ä½¿ç”¨LiteLLM)
  anthropic:
    model: "${ANTHROPIC_MODEL:anthropic/claude-3-5-sonnet}"
    api_key_env: "ANTHROPIC_API_KEY"
    max_tokens: 2000
    temperature: 0.7
    client_type: "litellm"

  google:
    model: "${GOOGLE_MODEL:google/gemini-2.0-flash-001}"
    api_key_env: "GOOGLE_API_KEY"
    max_tokens: 2000
    temperature: 0.7
    client_type: "litellm"

  local_llm:
    model: "${LOCAL_MODEL:llama2}"
    api_key_env: ""
    base_url: "${LOCAL_LLM_URL:http://localhost:11434}"
    max_tokens: 2000
    temperature: 0.7
    client_type: "litellm"

# æ¨¡å‹è·¯ç”±è§„åˆ™ (è‡ªåŠ¨æ£€æµ‹)
routing_rules:
  # OpenAIæ¨¡å‹å‰ç¼€ (ä½¿ç”¨æ ‡å‡†client)
  openai_prefixes:
    - "gpt-"
    - "text-davinci-"
    - "text-curie-"
    - "text-babbage-"
    - "text-ada-"
  
  # ç¬¬ä¸‰æ–¹æ¨¡å‹å‰ç¼€ (ä½¿ç”¨LiteLLM)
  litellm_prefixes:
    - "anthropic/"
    - "claude-"
    - "google/"
    - "gemini-"
    - "deepseek/"
    - "mistral/"
    - "meta/"
    - "cohere/"
  
  # é»˜è®¤è·¯ç”±ç­–ç•¥
  default_client: "openai"  # æœªçŸ¥æ¨¡å‹é»˜è®¤ä½¿ç”¨OpenAI client
  fallback_enabled: true    # å¯ç”¨fallbackæœºåˆ¶

# é»˜è®¤è®¾ç½® (åº”ç”¨äºæ‰€æœ‰æä¾›å•†)
defaults:
  timeout: 30
  retry_attempts: 3
  max_tokens: 2000
  temperature: 0.7
```

#### 4.3 MCP Server Configuration

```yaml
# config/mcp.yaml - MCPæœåŠ¡å™¨é…ç½®
servers:
  filesystem:
    type: "stdio"
    command: "npx"
    args: ["-y", "@modelcontextprotocol/server-filesystem", "${TINYAGENT_ROOT:.}"]
    env: {}
    description: "File system operations"
    enabled: true
    category: "file_operations"

  fetch:
    type: "stdio"
    command: "npx"
    args: ["-y", "@modelcontextprotocol/server-fetch"]
    env: {}
    description: "HTTP requests and web content"
    enabled: true
    category: "web_operations"

  sqlite:
    type: "stdio"
    command: "npx"
    args: ["-y", "@modelcontextprotocol/server-sqlite", "${TINYAGENT_DB:./data/tinyagent.db}"]
    env: {}
    description: "SQLite database operations"
    enabled: false
    category: "database_operations"

# æœåŠ¡å™¨åˆ†ç±»å®šä¹‰
categories:
  file_operations:
    description: "File system and document operations"
    priority: "high"
  
  web_operations:
    description: "Web requests and content fetching"
    priority: "medium"
    
  database_operations:
    description: "Database queries and operations"
    priority: "low"

# MCPå®¢æˆ·ç«¯è®¾ç½®
client:
  timeout: 30
  max_retries: 3
  tool_cache_duration: 300
  max_tools_per_server: 50
```

#### 4.4 Environment Variables (.env)

```bash
# .env - æ•æ„Ÿä¿¡æ¯å’Œç¯å¢ƒç‰¹å®šè®¾ç½®
# LLM API Keys
OPENAI_API_KEY=your-openai-api-key-here
OPENROUTER_API_KEY=your-openrouter-api-key-here

# LLM Configuration
OPENAI_MODEL=gpt-4
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet
LOCAL_LLM_URL=http://localhost:11434

# TinyAgent Settings
TINYAGENT_ROOT=C:/work/github/TinyAgent
TINYAGENT_DB=./data/tinyagent.db
LOG_LEVEL=INFO
LOG_FILE=

# OpenRouter Specific
OPENROUTER_REFERER=https://github.com/your-org/tinyagent
OPENROUTER_TITLE=TinyAgent

# Development Settings
TINYAGENT_PROFILE=development
TINYAGENT_DEBUG=false
```

### 5. Configuration Loading Strategy

#### 5.1 ConfigurationManager Enhancement

```python
class ConfigurationManager:
    """Enhanced configuration manager with hierarchical loading"""
    
    def __init__(self, 
                 config_dir: Path = None,
                 profile: str = None,
                 env_file: str = ".env"):
        self.config_dir = config_dir or self._find_config_dir()
        self.profile = profile or os.getenv("TINYAGENT_PROFILE", "development")
        self.env_file = env_file
        
        # Load .env file first
        self._load_dotenv()
    
    def _load_dotenv(self):
        """Load environment variables from .env file"""
        from dotenv import load_dotenv
        env_path = self.config_dir.parent / self.env_file
        if env_path.exists():
            load_dotenv(env_path)
    
    def load_configuration(self) -> TinyAgentConfig:
        """Load configuration with proper hierarchy"""
        # 1. Load defaults
        config = self._load_defaults()
        
        # 2. Load profile (if specified)
        if self.profile:
            profile_config = self._load_profile(self.profile)
            config = self._merge_configs(config, profile_config)
        
        # 3. Load user config
        user_config = self._load_user_config()
        config = self._merge_configs(config, user_config)
        
        # 4. Apply environment variables
        config = self._apply_env_substitution(config)
        
        return self._parse_config(config)
```

#### 5.2 Profile System

```yaml
# profiles/development.yaml - å¼€å‘ç¯å¢ƒé…ç½®
agent:
  name: "TinyAgent-Dev"
  max_iterations: 5

llm:
  active_provider: "openai"

mcp:
  servers:
    filesystem:
      enabled: true
    fetch:
      enabled: true
    sqlite:
      enabled: false

logging:
  level: "DEBUG"
```

```yaml
# profiles/production.yaml - ç”Ÿäº§ç¯å¢ƒé…ç½®
agent:
  name: "TinyAgent-Prod"
  max_iterations: 20

llm:
  active_provider: "openrouter"

mcp:
  servers:
    filesystem:
      enabled: true
    fetch:
      enabled: true
    sqlite:
      enabled: true

logging:
  level: "INFO"
  file: "logs/tinyagent.log"
```

### 6. Usage Examples

#### 6.1 Simple Usage (Environment Variables Only)

```bash
# Set environment variables
export OPENAI_API_KEY="your-key"
export TINYAGENT_PROFILE="development"

# Run with defaults
python -m tinyagent status
```

#### 6.2 Custom Configuration

```bash
# Use specific profile
python -m tinyagent --profile production status

# Use custom config directory
python -m tinyagent --config-dir ./custom-configs status
```

#### 6.3 Configuration Override Chain

```bash
# Override chain: defaults â†’ profile â†’ user-config â†’ env-vars
OPENAI_MODEL=gpt-3.5-turbo python -m tinyagent run "Hello"
```

### 7. Implementation Benefits

#### 7.1 Advantages
1. **Clear Separation**: Defaults, profiles, user configs, and env vars are clearly separated
2. **Easy Override**: Simple hierarchy makes it easy to override any setting
3. **Environment Agnostic**: Same codebase works across dev/staging/prod
4. **Secure**: Sensitive data stays in .env files
5. **Flexible**: Users can choose minimal or detailed configuration
6. **Backward Compatible**: Existing configs can be migrated gradually

#### 7.2 Migration Strategy
1. **Phase 1**: Implement new ConfigurationManager with backward compatibility
2. **Phase 2**: Create new config structure alongside old ones
3. **Phase 3**: Migrate existing configs to new structure
4. **Phase 4**: Deprecate old configuration methods

### 8. Configuration Validation

```python
class ConfigValidator:
    """Validates configuration completeness and correctness"""
    
    def validate(self, config: TinyAgentConfig) -> ValidationResult:
        errors = []
        warnings = []
        
        # Validate LLM configuration
        if not self._check_api_key(config.llm):
            errors.append("LLM API key not found or invalid")
        
        # Validate MCP servers
        for server in config.mcp.servers:
            if server.enabled and not self._check_server_availability(server):
                warnings.append(f"MCP server {server.name} may not be available")
        
        return ValidationResult(errors=errors, warnings=warnings)
```

### 9. LiteLLM Integration Strategy

#### 9.1 Implementation Architecture

```python
class LLMClientRouter:
    """æ™ºèƒ½LLMå®¢æˆ·ç«¯è·¯ç”±å™¨"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.routing_rules = config.routing_rules
    
    def detect_client_type(self, model_name: str) -> str:
        """æ ¹æ®æ¨¡å‹åç§°æ£€æµ‹åº”ä½¿ç”¨çš„å®¢æˆ·ç«¯ç±»å‹"""
        # æ£€æŸ¥æ˜¾å¼é…ç½®
        if hasattr(self.config, 'client_type'):
            return self.config.client_type
        
        # è‡ªåŠ¨æ£€æµ‹åŸºäºå‰ç¼€
        for prefix in self.routing_rules.litellm_prefixes:
            if model_name.startswith(prefix):
                return "litellm"
        
        for prefix in self.routing_rules.openai_prefixes:
            if model_name.startswith(prefix):
                return "openai"
        
        return self.routing_rules.default_client
    
    def create_model(self, model_name: str, **kwargs):
        """åˆ›å»ºé€‚å½“çš„æ¨¡å‹å®ä¾‹"""
        client_type = self.detect_client_type(model_name)
        
        if client_type == "litellm":
            from agents.extensions.models.litellm_model import LitellmModel
            return LitellmModel(
                model=model_name,
                api_key=kwargs.get('api_key'),
                base_url=kwargs.get('base_url'),
                **kwargs
            )
        else:
            # ä½¿ç”¨æ ‡å‡†OpenAIæ¨¡å‹
            return model_name  # OpenAI Agents SDKå¤„ç†
```

#### 9.2 ä¾èµ–å’Œå®‰è£…è¦æ±‚

```bash
# æ ¸å¿ƒä¾èµ–
pip install "openai-agents[litellm]"

# ç¡®ä¿LiteLLMæ”¯æŒ
pip install litellm>=1.0.0

# å¯é€‰ï¼šç‰¹å®šæä¾›å•†SDK
pip install anthropic  # Anthropic models
pip install google-generativeai  # Google models
```

#### 9.3 é…ç½®è¿ç§»ç­–ç•¥

1. **è‡ªåŠ¨æ£€æµ‹ç°æœ‰é…ç½®**
2. **åŸºäºæ¨¡å‹åç§°æ¨æ–­å®¢æˆ·ç«¯ç±»å‹**
3. **ä¿æŒå‘åå…¼å®¹æ€§**
4. **æ¸è¿›å¼è¿ç§»è·¯å¾„**

#### 9.4 é”™è¯¯å¤„ç†å’ŒFallback

```python
class LLMClientManager:
    """LLMå®¢æˆ·ç«¯ç®¡ç†å™¨ï¼ŒåŒ…å«é”™è¯¯å¤„ç†å’Œfallback"""
    
    async def create_agent_with_fallback(self, config):
        """åˆ›å»ºAgentï¼ŒåŒ…å«fallbackæœºåˆ¶"""
        try:
            # å°è¯•é¦–é€‰æ¨¡å‹
            return await self._create_agent(config.primary_model)
        except UnsupportedModelError:
            # å›é€€åˆ°OpenAIå…¼å®¹æ¨¡å¼
            logger.warning(f"Falling back to OpenAI-compatible mode for {config.primary_model}")
            return await self._create_agent_openai_mode(config.fallback_model)
        except Exception as e:
            # æœ€ç»ˆå›é€€
            logger.error(f"Model creation failed: {e}")
            return await self._create_basic_agent()
```

### 10. LiteLLM é›†æˆå®ç°çŠ¶æ€

#### é˜¶æ®µ1ï¼šLiteLLMé›†æˆ - âœ… **å·²å®Œæˆ (2025-06-01)**
1. **å®‰è£…LiteLLMä¾èµ–**: âœ… **å®Œæˆ** - `openai-agents[litellm]>=0.0.16`
2. **å®ç°æ¨¡å‹è·¯ç”±å™¨**: âœ… **å®Œæˆ** - è‡ªåŠ¨æ£€æµ‹æ¨¡å‹å‰ç¼€å¹¶è·¯ç”±åˆ°æ­£ç¡®å®¢æˆ·ç«¯
3. **æ›´æ–°Agentåˆ›å»ºé€»è¾‘**: âœ… **å®Œæˆ** - æ”¯æŒ `LitellmModel` å’Œä¼ ç»Ÿå­—ç¬¦ä¸²æ¨¡å‹
4. **æµ‹è¯•ç¬¬ä¸‰æ–¹æ¨¡å‹**: âœ… **å®Œæˆ** - Google Gemini 2.0 Flash æµ‹è¯•æˆåŠŸ

**å®ç°æˆæœ**:
- âœ… æ”¯æŒ100+ç¬¬ä¸‰æ–¹LLMæ¨¡å‹ï¼ˆGoogle, Anthropic, DeepSeek, Mistralç­‰ï¼‰
- âœ… è‡ªåŠ¨æ¨¡å‹è·¯ç”±åŸºäºå‰ç¼€æ£€æµ‹ (`google/`, `anthropic/`, `deepseek/`ç­‰)
- âœ… ä¿æŒå¯¹OpenAIåŸç”Ÿæ¨¡å‹çš„å®Œå…¨å…¼å®¹æ€§
- âœ… OpenRouteré›†æˆå·¥ä½œæ­£å¸¸
- âœ… æˆåŠŸè°ƒç”¨Google Gemini 2.0 Flashå¹¶è¿”å›æ­£ç¡®å“åº”

**æµ‹è¯•éªŒè¯**:
```bash
# æˆåŠŸæµ‹è¯•æ¡ˆä¾‹
python -m tinyagent.cli.main run "Hello! Can you introduce yourself?"

# æ—¥å¿—æ˜¾ç¤ºæ­£ç¡®è·¯ç”±
LiteLLM completion() model= google/gemini-2.0-flash-001; provider = openrouter
HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
```

#### é˜¶æ®µ2ï¼šé…ç½®å¢å¼º - ğŸ“‹ **å¾…å®ç°**
1. **æ·»åŠ æ¨¡å‹ç±»å‹æ£€æµ‹** - éƒ¨åˆ†å®Œæˆï¼ˆåŸºç¡€æ£€æµ‹å·²å®ç°ï¼‰
2. **å®ç°è‡ªåŠ¨è·¯ç”±è§„åˆ™** - éƒ¨åˆ†å®Œæˆï¼ˆé™æ€è§„åˆ™å·²å®ç°ï¼‰
3. **å¢å¼ºé”™è¯¯å¤„ç†** - éœ€è¦æ”¹è¿›
4. **æ·»åŠ æ€§èƒ½ç›‘æ§** - å¾…å®ç°

#### é˜¶æ®µ3ï¼šç”Ÿäº§ä¼˜åŒ– - ğŸ“‹ **è®¡åˆ’ä¸­**
1. **æ¨¡å‹ç¼“å­˜æœºåˆ¶** - å¾…å®ç°
2. **è´Ÿè½½å‡è¡¡æ”¯æŒ** - å¾…å®ç°
3. **æˆæœ¬ä¼˜åŒ–ç­–ç•¥** - å¾…å®ç°
4. **ç›‘æ§å’Œå‘Šè­¦** - å¾…å®ç°

### 11. å·²çŸ¥é—®é¢˜å’Œä¿®å¤è®¡åˆ’

#### 11.1 å½“å‰å·²çŸ¥é—®é¢˜
1. **aiohttpè¿æ¥æœªå…³é—­è­¦å‘Š**: 
   ```
   ERROR - Unclosed client session
   ERROR - Unclosed connector
   ```
   - çŠ¶æ€: ğŸ”§ **éœ€è¦ä¿®å¤** - ä¸å½±å“åŠŸèƒ½ä½†éœ€è¦æ¸…ç†
   - è§£å†³æ–¹æ¡ˆ: åœ¨agent.pyä¸­æ·»åŠ é€‚å½“çš„è¿æ¥å…³é—­é€»è¾‘

#### 11.2 å®ç°å®ŒæˆçŠ¶æ€æ€»ç»“

| ç»„ä»¶ | çŠ¶æ€ | å®ç°æ—¥æœŸ | å¤‡æ³¨ |
|------|------|----------|------|
| æ¨¡å‹æ£€æµ‹é€»è¾‘ | âœ… å®Œæˆ | 2025-06-01 | è‡ªåŠ¨æ£€æµ‹ç¬¬ä¸‰æ–¹æ¨¡å‹å‰ç¼€ |
| LitellmModelé›†æˆ | âœ… å®Œæˆ | 2025-06-01 | æ”¯æŒ100+æ¨¡å‹æä¾›å•† |
| OpenRouterè·¯ç”± | âœ… å®Œæˆ | 2025-06-01 | è‡ªåŠ¨æ·»åŠ openrouter/å‰ç¼€ |
| é…ç½®ç³»ç»Ÿå…¼å®¹ | âœ… å®Œæˆ | 2025-06-01 | æ— éœ€ä¿®æ”¹ç°æœ‰é…ç½® |
| Google Geminiæµ‹è¯• | âœ… å®Œæˆ | 2025-06-01 | æˆåŠŸè°ƒç”¨å¹¶è¿”å›å“åº” |
| å‘åå…¼å®¹æ€§ | âœ… å®Œæˆ | 2025-06-01 | OpenAIæ¨¡å‹ç»§ç»­æ­£å¸¸å·¥ä½œ |

### 12. Next Steps for Implementation

1. **Install LiteLLM**: Add `openai-agents[litellm]` to requirements.txt âœ… **å®Œæˆ**
2. **Enhance ConfigurationManager**: Implement model routing logic âœ… **å®Œæˆ**
3. **Update Agent Creation**: Add LitellmModel support âœ… **å®Œæˆ**
4. **Create Model Router**: Implement automatic client detection âœ… **å®Œæˆ**
5. **Add Validation**: Implement model compatibility validation âœ… **åŸºç¡€å®Œæˆ**
6. **Migration Guide**: Create guide for migrating to new model support âœ… **æ–‡æ¡£å·²æ›´æ–°**
7. **Testing**: Comprehensive testing with multiple model providers âœ… **Google Geminiæµ‹è¯•å®Œæˆ**
8. **Fix Connection Issues**: Resolve aiohttp connection warnings ğŸ”§ **å¾…ä¿®å¤**

### 13. æ¶æ„ä¼˜åŠ¿éªŒè¯

é€šè¿‡LiteLLMé›†æˆçš„æˆåŠŸå®ç°ï¼ŒéªŒè¯äº†ä»¥ä¸‹æ¶æ„å†³ç­–çš„æ­£ç¡®æ€§ï¼š

1. **æ¨¡å—åŒ–è®¾è®¡**: LLMæä¾›å•†æŠ½è±¡å±‚ä½¿å¾—æ·»åŠ æ–°æ¨¡å‹æ”¯æŒå˜å¾—ç®€å•
2. **é…ç½®é©±åŠ¨**: æ— éœ€ä»£ç æ›´æ”¹å³å¯åˆ‡æ¢æ¨¡å‹
3. **è‡ªåŠ¨è·¯ç”±**: åŸºäºæ¨¡å‹åç§°çš„æ™ºèƒ½è·¯ç”±å‡å°‘äº†é…ç½®å¤æ‚æ€§
4. **å‘åå…¼å®¹**: ç°æœ‰OpenAIé›†æˆç»§ç»­æ— ç¼å·¥ä½œ
5. **å¯æ‰©å±•æ€§**: æ¶æ„æ”¯æŒæœªæ¥æ·»åŠ æ›´å¤šæ¨¡å‹æä¾›å•†

This design provides a clean, hierarchical, and flexible configuration system that scales from simple usage to complex enterprise deployments, with proven LiteLLM integration supporting 100+ models.

---

*This document captures the system architecture and design patterns used in the project.* 