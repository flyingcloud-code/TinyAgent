Notepad Name: envsetup
## Basic Environment
- You are under windows OS, not linux. 
- Your default shell is windows commandline
- uv tool is intalled
- .venv installed at project root folder
- .venv\Script\Actiavate to activate python
- uv pip install <<package>> to get expected module
### trouble shoot
Some times you on in conda env of "ak_dev", need to follow below instruciton to use local .venv.
after that you can get into correct env
```
(ak_dev) PS C:\work\github\TinyAgent> conda deactivate
(base) PS C:\work\github\TinyAgent> .\.venv\Scripts\activate
(base) (TinyAgent) PS C:\work\github\TinyAgent> uv pip list | findstr openai
openai                        1.82.1
openai-agents                 0.0.16
(base) (TinyAgent) PS C:\work\github\TinyAgent> python --version
Python 3.11.11
```
## set up cursor rules
- **only need setup once**
- [x] Current Status: **Completed*
- cd c:\wyang11\github\TinyAgent
- mkdir -p .cursor
- cp ../CursorRIPER/src/.cursor ./
- for file in *.md; do mv "$file" "${file%.md}.mdc"; done

## How to get latest date
- Under power shell, execute command: date
- Under windows command, execute command: date /T

## Context 7 MCP usage
- for open agent sdk, search: "OpenAI Agents Python"
- for offical openai python module, search: OpenAI Python
- for litellm python module, search: litellm

## LiteLLM Multi-Model Support Configuration
### Overview
TinyAgent now supports 100+ LLM models via LiteLLM integration with automatic model routing:
- **OpenAI Models**: gpt-4, gpt-3.5-turbo → Standard OpenAI client
- **Third-Party Models**: google/, anthropic/, deepseek/ → LiteLLM client
- **Automatic Detection**: Based on model name prefix

### Installation
```bash
# Install OpenAI Agents SDK with LiteLLM support
uv pip install "openai-agents[litellm]>=0.0.16"
```

### Supported Model Prefixes (Auto-routed to LiteLLM)
- `google/` - Google models (Gemini)
- `anthropic/` - Anthropic models (Claude)
- `deepseek/` - DeepSeek models
- `mistral/` - Mistral models
- `meta/` - Meta models (Llama)
- `cohere/` - Cohere models
- `replicate/` - Replicate models
- `azure/` - Azure models
- `vertex_ai/` - Vertex AI models

### Configuration Examples
```yaml
# In tinyagent/configs/defaults/llm_providers.yaml
providers:
  openrouter:
    model: "google/gemini-2.0-flash-001"  # Auto-detected as LiteLLM
    api_key_env: "OPENROUTER_API_KEY"
    base_url: "https://openrouter.ai/api/v1"
    temperature: 0.7
  
  openai:
    model: "gpt-4"  # Auto-detected as OpenAI native
    api_key_env: "OPENAI_API_KEY"
    temperature: 0.7
```

### Model Routing Logic
```python
# Automatic model detection in TinyAgent
def _should_use_litellm(model_name: str) -> bool:
    third_party_prefixes = [
        'google/', 'anthropic/', 'deepseek/', 'mistral/', 
        'meta/', 'cohere/', 'replicate/', 'azure/', 'vertex_ai/'
    ]
    return any(model_name.startswith(prefix) for prefix in third_party_prefixes)

# Model instance creation
if _should_use_litellm(model_name):
    # Create LitellmModel for third-party models
    model = LitellmModel(model=model_name, api_key=api_key, base_url=base_url)
else:
    # Use string for OpenAI native models
    model = model_name
```

### Usage Examples
```bash
# Test Google Gemini via OpenRouter
python -m tinyagent.cli.main run "Hello from Google Gemini!"

# Test different models by changing configuration
# Model will be auto-routed based on prefix
```

## default llm provider in .env file
- should use <<openrouter>>
- default model of openrouter should be: <<google/gemini-2.0-flash-001>> (LiteLLM auto-routing)
- Alternative: <<deepseek/deepseek-chat-v3-0324>> (also LiteLLM auto-routing)
- <<NOTE>> Third-party models automatically use LiteLLM via model prefix detection

### example to use openrouter APIs with openai sdk
```
from openai import OpenAI

client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key="<OPENROUTER_API_KEY>",
)

completion = client.chat.completions.create(
  extra_headers={
    "HTTP-Referer": "<YOUR_SITE_URL>", # Optional. Site URL for rankings on openrouter.ai.
    "X-Title": "<YOUR_SITE_NAME>", # Optional. Site title for rankings on openrouter.ai.
  },
  extra_body={},
  model="google/gemini-2.0-flash-001",  # Will auto-route to LiteLLM
  messages=[
    {
      "role": "user",
      "content": "What is the meaning of life?"
    }
  ]
)
print(completion.choices[0].message.content)
```

### LiteLLM Integration Benefits
- ✅ **100+ Models**: Support for Google, Anthropic, DeepSeek, Mistral, etc.
- ✅ **Automatic Routing**: No configuration changes needed
- ✅ **Unified Interface**: Same API for all models
- ✅ **Cost Optimization**: Easy model switching for cost/performance
- ✅ **Backward Compatible**: Existing OpenAI models continue to work

## TinyAgent Configuration System (Updated 2025-06-01)
### New Hierarchical Configuration Architecture
TinyAgent now uses a sophisticated hierarchical configuration system with the following priority order:
1. **Environment Variables (.env file)** - HIGHEST PRIORITY
2. **User Configuration (config/)** - HIGH PRIORITY  
3. **Profile Configurations (profiles/)** - MEDIUM PRIORITY
4. **Default Configurations (defaults/)** - LOWEST PRIORITY

### Configuration Directory Structure
```
tinyagent/configs/
├── defaults/                    # Default configs (read-only, shipped with code)
│   ├── agent.yaml              # Agent default settings
│   ├── llm_providers.yaml      # LLM provider defaults
│   └── mcp_servers.yaml        # MCP server defaults
├── profiles/                    # Profile configs (examples and templates)
│   ├── development.yaml        # Development environment
│   ├── production.yaml         # Production environment
│   ├── openrouter.yaml         # OpenRouter example
│   └── local_llm.yaml          # Local LLM example
├── config/                      # User custom configs (highest priority)
│   ├── agent.yaml              # User agent config
│   ├── llm.yaml                # User LLM config
│   └── mcp.yaml                # User MCP config
└── env.template                # Environment variables template
```

### Environment Variables Setup
1. **Copy template**: `cp tinyagent/configs/env.template .env`
2. **Edit .env file** with your actual values:
   ```bash
   # Required - choose your LLM provider
   OPENAI_API_KEY=your-openai-api-key-here
   # OR
   OPENROUTER_API_KEY=your-openrouter-api-key-here
   
   # Optional settings
   TINYAGENT_PROFILE=development
   TINYAGENT_ROOT=C:/work/github/TinyAgent
   LOG_LEVEL=INFO
   ```

### Using Profiles
- **Default profile**: `development` (automatic)
- **Set via environment**: `export TINYAGENT_PROFILE=production`
- **Set via CLI**: `python -m tinyagent --profile production status`
- **List available profiles**: `python -m tinyagent list-profiles`

### Key Configuration Commands
```bash
# Check status with default profile
python -m tinyagent status

# Check status with specific profile
python -m tinyagent status --profile production

# List available profiles
python -m tinyagent list-profiles

# Use verbose mode for detailed info
python -m tinyagent status -v

# Run with specific profile
python -m tinyagent --profile openrouter run "Hello"
```

### Dependencies Added
- **python-dotenv**: For .env file support
- Already installed via: `uv pip install python-dotenv`

### Migration Notes
- Old configuration files remain supported for backward compatibility
- New hierarchical system takes precedence when available
- `.env` file is optional but recommended for sensitive data
- Profile system allows easy switching between environments 